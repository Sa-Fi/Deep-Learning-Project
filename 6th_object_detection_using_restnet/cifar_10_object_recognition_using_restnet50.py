# -*- coding: utf-8 -*-
"""Cifar_10_object_recognition_using_restnet50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qgUAzQWCEXCeGdmKAjCtyND2khEgDsUx
"""

# configuring the path of Kaggle.json file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# daatset api
!kaggle competitions download -c cifar-10

!ls



"""Extral all the zip and 7z file

"""

# extracting the compessed Dataset
from zipfile import ZipFile
dataset = '/content/cifar-10.zip'

with ZipFile(dataset,'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

!ls

!pip install py7zr

import py7zr

archive = py7zr.SevenZipFile('/content/train.7z', mode='r')
archive.extractall()     #archive.extractall(path='/content/Training Data')
archive.close()

!ls

"""Importing the all the library dependencies"""

import os 
import numpy as np 
import matplotlib.pyplot as plt 
import matplotlib.image  as mpimg
from PIL import Image
from sklearn.model_selection import train_test_split
import pandas as pd

filenames = os.listdir('/content/train')
print(type(filenames))
print(len(filenames))

print(filenames[0:5]) # show the first 5 file 
print(filenames[-5:]) #show the last 5 file

"""Label processing """

label_df = pd.read_csv('trainLabels.csv')
print(label_df)
print('label_df shape',label_df.shape)

(label_df[label_df['id']==775])

print(label_df.head(10))
print('\n')
print(label_df.tail(10))

label_df['label'].value_counts()

labels_dictionary = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}
labels =  [labels_dictionary[i] for i in label_df['label']]
print(labels[0:5])
print(labels[-5:])

"""Displaying the sample image """

import cv2 
from google.colab.patches import cv2_imshow

img = cv2.imread('/content/train/7796.png')
cv2_imshow(img)

label_df[label_df['id']==7796]

img = cv2.imread('/content/train/5000.png')
cv2_imshow(img)
label_df[label_df['id']==5000]

id_list = list(label_df['id'])
print(id_list[0:5])
print(id_list[-5:])

"""Image processing """

# convert image to the numpy arrays 
train_data_folder = '/content/train/'


for id in id_list:
  img_path = train_data_folder + str(id) + '.png'
  print(img_path)
  if id >= 5:
      break

data = []
for id in id_list:
  image = Image.open(train_data_folder + str(id) + '.png')
  image = np.array(image)
  data.append(image)

print(type(data))
print(len(data))

print("type of each data",type(data[0]))
print('shape of each data ',data[0].shape)

data[0]

# convert the images and label into numpy arrays 
x = np.array(data)
y = np.array(labels)

print("type of x ",type(x))
print('type of y ',type(y))

print('shape of x ',x.shape)
print('shape of y ',y.shape)

"""train test split """

x_train ,x_test, y_train,y_test = train_test_split(x,y,test_size = 0.2, random_state = 2)
print("x shape ",x.shape)
print("x_train shape ",x_train.shape)
print("x_test shape ",x_test.shape)
print('\n\n')
print("y shape ",y.shape)
print("y_train shape ",y_train.shape)
print("y_test shape ",y_test.shape)

"""Scaling the data """

x_train_scaled = x_train/255
x_test_scaled = x_test/255
print("x_train before scaling ",x_train[0])
print("x_train after scaling ",x_train_scaled[0].round(2))

"""Building the CNN model """

import tensorflow as tf
from tensorflow import keras 
from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout

from keras.engine.sequential import Sequential
model = Sequential()
#1st layer 
model.add(Conv2D(32,kernel_size =(3,3),activation='relu',input_shape=(32,32,3)))
model.add(MaxPooling2D(pool_size=(2,2)))


#2nd layer
model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

#flatten layer 
model.add(Flatten())

#Dense layer 
model.add(Dense(128,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation= 'relu'))
model.add(Dropout(0.5))

model.add(Dense(10,activation='softmax'))

#compile the model
model.compile(optimizer='adam',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])

x_train_scaled_reshape = x_train_scaled.reshape(-1,32,32,3)
tf.config.experimental_run_functions_eagerly(True)
history = model.fit(x_train_scaled_reshape,
                    y_train,
                    validation_split=0.1,
                    epochs=10)

num_of_classes = 10

# setting up the layers of Neural Network

model = keras.Sequential([
    
    keras.layers.Flatten(input_shape=(32,32,3)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(num_of_classes, activation='softmax')
])
# compile the neural network
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['acc'])
# training the neural network
model.fit(x_train_scaled, y_train, validation_split=0.1, epochs=5)

"""RestNet50"""

import tensorflow
from tensorflow.keras import Sequential,models,layers,optimizers
from tensorflow.keras.layers import Dense,Dropout,Flatten,BatchNormalization
from tensorflow.keras.models import load_model,Model
from tensorflow.keras.applications.resnet50 import ResNet50

CNN_base = ResNet50(weights = 'imagenet',
                            include_top= False,
                    input_shape=(256,256,3) )
CNN_base.summary()

res_model = models.Sequential()
res_model.add(layers.UpSampling2D((2,2))) 
res_model.add(layers.UpSampling2D((2,2))) 
res_model.add(layers.UpSampling2D((2,2))) 

res_model.add(CNN_base)

res_model.add(layers.Flatten())

res_model.add(layers.BatchNormalization())
res_model.add(layers.Dense(128,activation ='relu'))
res_model.add(layers.Dropout(0.5))

res_model.add(layers.BatchNormalization())
res_model.add(layers.Dense(64,activation ='relu'))
res_model.add(layers.Dropout(0.5))

res_model.add(layers.BatchNormalization())
res_model.add(layers.Dense(10,activation ='softmax'))


# compile the resnetmodel resmodel
res_model.compile(optimizer=optimizers.RMSprop(lr=2e-5), loss='sparse_categorical_crossentropy', metrics=['acc'])

#x_train_scaled_reshape = np.reshape(x_train_scaled,[-1,32,32,3])
history = res_model.fit(x_train_scaled, y_train, validation_split=0.1, epochs=5)

loss, accuracy = res_model.evaluate(x_test_scaled, x_test)
print('Test Accuracy =', accuracy)

h = history
plt.plot(h.history['acc'],label = 'train accuracy ')
plt.plot(h.history['val_acc'],label= 'validation accuracy')
plt.legend()
plt.show()

h = history
plt.plot(h.history['loss'],label = 'train loss ')
plt.plot(h.history['val_loss'],label= 'validation loss')
plt.legend()
plt.show()

model_deploy=res_model.save('/content/drive/MyDrive/Deep learning Save model/Cifar_10_object_detection_using_resnet50.h5')